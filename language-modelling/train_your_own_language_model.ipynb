{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9dZTKHa6Cm5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Setup cell -- Run me first!\n",
        "\n",
        "!pip install --upgrade huggingface_hub datasets transformers accelerate>=0.21.0 better-profanity --quiet\n",
        "!apt install git-lfs --quiet\n",
        "\n",
        "import transformers\n",
        "from better_profanity import profanity as p\n",
        "import builtins\n",
        "import io\n",
        "import textwrap\n",
        "\n",
        "\n",
        "class PrintWrapper(io.StringIO):\n",
        "  def __call__(self, *args, **kwargs):\n",
        "    args = list(args)\n",
        "    if len(args) > 0:\n",
        "        if isinstance(args[0], str):\n",
        "            args[0] = p.censor(args[0])\n",
        "            args = tuple(args)\n",
        "    return builtins.print(*args, **kwargs)\n",
        "\n",
        "print = PrintWrapper()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICQvq88P6G0s",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Logging in to Hugging Face (used later to let you save your model!)\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KaLyUPAgvuW"
      },
      "source": [
        "# Fine-tuning a language model\n",
        "\n",
        "What is fine tuning? Think of sculpting a large block of clay into a specific structure. The block represents a foundation for our sculpture -- something that has many potential uses, and also shapable into whatever we can imagine. Carving out our sculpture is then the process of fine-tuning: taking our generally useful material and giving it a specific purpose.\n",
        "\n",
        "In the context of machine learning, a similar concept to this foundation exists: people with lots of time and money have trained some really, really large models (large = number of free parameters) on absolutely massive datasets. Models trained at this scale seem to be able to match human performance on many benchmarks, possibly by capturing some generalisable concepts about language and knowledge. As such, they've been termed \"**foundation models**\", which comes from their ability to serve as a good foundation for fine-tuning on specific tasks.\n",
        "\n",
        "A common starting point for language tasks is [OpenAI's GPT2](https://huggingface.co/gpt2), which has been trained on a large body of text scraped from the internet. There are also many other popular models from various teams and companies that are openly available on the [HuggingFace model hub](https://huggingface.co/models) that would work well for a variety of use cases (e.g. image generation, text classification, language translation etc.).\n",
        "\n",
        "If you've read a bit about [ChatGPT](https://chat.openai.com) and are wondering why we're not using GPT3/3.5 or GPT4, the answer is that neither model is *open-source*, meaning that the model parameters were not released to the general public. Even if they were though, they would likely not fit in memory since they're just that large -- I didn't see a specific number, but GPT3 size estimates are from 300-800GB (for context, this notebook has a GPU with memory of 12GB, and the best cards out there only go to 50-100GB, so you'd have to link many GPUs together to load the model!)\n",
        "\n",
        "So, how is fine-tuning done? Broadly speaking, there are a few key questions we need to answer:\n",
        "- What's the *goal* of my task?\n",
        "- What *data* can I use to give examples of this task?\n",
        "- Which *model* should I choose to fine-tune?\n",
        "\n",
        "In this notebook, we'll fix the answer to the first question as *text generation*. The goal there is usually to predict the next character/word/sentence in a sequence to match as close as possible with examples of text that you give to the model. However, the *type* of text we generate is completely up to you -- it all depends on the examples you show the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22EtDDnr2q5B",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Select your model! We'll start with a \"distilled\" version of GPT-2 (click the drop-down text box for more choices, or look [on the huggingface hub](https://huggingface.co/models?pipeline_tag=text-generation))\n",
        "model_checkpoint = \"distilgpt2\" #@param {type:\"string\"}\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n",
        "from transformers import pipeline\n",
        "pipe = pipeline(model=model_checkpoint, tokenizer=tokenizer, device=\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "PLMcf8YoBw59"
      },
      "outputs": [],
      "source": [
        "#@title Let's see how well our model can autocomplete text (change it to whatever you want!):\n",
        "text_to_complete = \"the quick brown fox jumped\" #@param {type: \"string\"}\n",
        "num_predictions = 3 #@param {type: \"integer\"}\n",
        "max_new_tokens = 100 #@param {type: \"integer\"}\n",
        "for i, a in enumerate(pipe(text_to_complete, num_return_sequences=num_predictions, return_full_text=True, max_new_tokens=max_new_tokens)):\n",
        "    print(f'\\r \\n Prediction {i+1}: ' + 2*'\\n' + a['generated_text']+ '\\n', end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T-kmnZWIuQo"
      },
      "source": [
        "# Choosing your dataset\n",
        "\n",
        "There's a couple of ways we've let you input data for this model to be trained on. For now, we've pre-prepared some datasets from HuggingFace on [Amazon reviews](https://huggingface.co/datasets/SetFit/amazon_reviews_multi_en) and [recipes](https://huggingface.co/datasets/m3hrdadfi/recipe_nlg_lite), which you can choose from the drop-down menu.\n",
        "\n",
        "It's also set up to try to download any HuggingFace dataset, so you can even try your own!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8C24mcH03fK",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Run me when you've selected your dataset!\n",
        "huggingface_dataset_name = 'amazon_reviews_multi_en' #@param [\"amazon_reviews_multi_en\", \"m3hrdadfi/recipe_nlg_lite\"] {allow-input: true}\n",
        "from datasets import load_dataset\n",
        "\n",
        "\n",
        "if huggingface_dataset_name == \"amazon_reviews_multi_en\":\n",
        "    datasets = load_dataset(\"SetFit/amazon_reviews_multi_en\")\n",
        "    # def filter_condition(data):\n",
        "    #     condition = data[\"stars\"] == 5  # edit this line!\n",
        "    #     return condition\n",
        "    # datasets = datasets.filter(filter_condition)\n",
        "\n",
        "\n",
        "elif huggingface_dataset_name == \"brianarbuckle/cocktail_recipes\":\n",
        "    datasets = load_dataset(\"brianarbuckle/cocktail_recipes\")\n",
        "    rs = []\n",
        "    for partition in datasets:\n",
        "        for data in datasets[partition]:\n",
        "            recipe_text = f\"Cocktail: {data['title']}\\n\\n\"\n",
        "\n",
        "            recipe_text += \"Ingredients:\\n\"\n",
        "            for ingredient in data['ingredients']:\n",
        "                recipe_text += f\"- {ingredient}\\n\"\n",
        "\n",
        "            recipe_text += \"\\nDirections:\\n\"\n",
        "            for step, direction in enumerate(data['directions'], start=1):\n",
        "                recipe_text += f\"Step {step}: {direction}\\n\"\n",
        "            rs.append(recipe_text)\n",
        "        datasets[partition] = datasets[partition].add_column('long_recipe', rs)\n",
        "\n",
        "elif huggingface_dataset_name == \"m3hrdadfi/recipe_nlg_lite\":\n",
        "    datasets = load_dataset(\"m3hrdadfi/recipe_nlg_lite\")\n",
        "    print(\"download complete!\")\n",
        "    for partition in datasets:\n",
        "        rs= []\n",
        "\n",
        "        for data in datasets[partition]:\n",
        "            full_recipe = f\"Recipe: {data['name']}\\n\\n\"\n",
        "            full_recipe += f\"Description: {data['description']}\\n\\n\"\n",
        "            full_recipe += f\"Ingredients:\\n{data['ingredients']}\\n\\n\"\n",
        "            full_recipe += f\"Steps:\\n{data['steps']}\\n\\n\"\n",
        "            rs.append(full_recipe)\n",
        "        datasets[partition] = datasets[partition].add_column('full_recipe', rs)\n",
        "else:\n",
        "    try:\n",
        "        datasets = load_dataset(huggingface_dataset_name)\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        msg = f\"Error: {huggingface_dataset_name} not found on HuggingFace datasets\"\n",
        "        raise Exception(msg)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9ewjwIzXE9b"
      },
      "source": [
        "## Uploading your own data as a text file\n",
        "\n",
        "### NOTE: THIS WILL OVERRIDE THE PREVIOUS CELL!\n",
        "\n",
        "If you have your own data to hand, you can upload it to this Colab notebook and provide the filename to load it in! Just copy and paste your text in a `.txt` document, then click into the \"Files\" tab in the top right, and drag it into the base directory (not into a folder)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4Ad8xNANXFqN"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "your_file_name = 'game-of-thrones.txt' #@param {type: \"string\"}\n",
        "\n",
        "try:\n",
        "    with open(your_file_name, 'r+') as file:\n",
        "        text = [x+'\\n' for x in file.read().splitlines() if x.strip() !='']\n",
        "    my_data = dict(number=list(range(len(text))), line=text)\n",
        "    datasets = DatasetDict(dict(train=Dataset.from_dict(my_data)))\n",
        "\n",
        "except Exception as e:\n",
        "    raise(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HH6R-dopegGp"
      },
      "outputs": [],
      "source": [
        "#@title See some example paragraphs from your training data:\n",
        "\n",
        "from datasets import ClassLabel\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def print_newlines(df):\n",
        "    return display( HTML( df.to_html().replace(\"\\\\n\",\"<br>\") ) )\n",
        "\n",
        "def show_random_elements(dataset, num_examples=5):\n",
        "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset)-1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset)-1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "    print_newlines(df)\n",
        "\n",
        "show_random_elements(datasets[\"train\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KS0vWcut35lF"
      },
      "outputs": [],
      "source": [
        "#@title Select name of the column in your dataset that contains the text you want to use as training material\n",
        "\n",
        "text_column = 'text'  #@param {type:\"string\"}\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[text_column])\n",
        "\n",
        "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=datasets['train'].column_names)\n",
        "# block_size = tokenizer.model_max_length\n",
        "block_size = 100\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "\n",
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1eLcHkH34R7p"
      },
      "outputs": [],
      "source": [
        "#@title Set up your hyperparameters for training, then run this cell!\n",
        "from transformers import AutoModelForCausalLM\n",
        "from datasets import Dataset\n",
        "model = AutoModelForCausalLM.from_pretrained(model_checkpoint)\n",
        "from transformers import Trainer, TrainingArguments\n",
        "my_model_name = \"my-great-gpt2-review-model-katie\" #@param {type:\"string\"}\n",
        "learning_rate = 0.000891 #@param {type:\"slider\", min:0.000001, max:0.01, step:0.00001}\n",
        "weight_decay=0.01\n",
        "num_train_epochs=0.3 #@param {type:\"slider\", min:0.1, max:20, step:0.1}\n",
        "training_args = TrainingArguments(\n",
        "    my_model_name,\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    weight_decay=weight_decay,\n",
        "    push_to_hub=True,\n",
        "    num_train_epochs=num_train_epochs,\n",
        ")\n",
        "if \"validation\" in datasets:\n",
        "    eval_dataset=lm_datasets[\"validation\"]\n",
        "else:\n",
        "    eval_dataset=Dataset.from_dict(lm_datasets[\"train\"][:100])\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets[\"train\"],\n",
        "    eval_dataset=eval_dataset\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uB-pj9LNlOjF"
      },
      "outputs": [],
      "source": [
        "#@title You can evaluate the model \"perplexity\" -- lower should mean it can generate things more like the data you trained on\n",
        "import math\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KD1OKf-P5H1d"
      },
      "outputs": [],
      "source": [
        "#@title Run the training!\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5zgGN-og5I-Y"
      },
      "outputs": [],
      "source": [
        "#@title Hopefully, the perplexity should be less -- run this cell to find out!\n",
        "import math\n",
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "T7KAoK0Eq30U"
      },
      "outputs": [],
      "source": [
        "#@title Try out your fine-tuned model!\n",
        "pipe = pipeline(model=trainer.model.to('cpu'), tokenizer=tokenizer, task='text-generation')\n",
        "text_to_complete = \"How do i make ramen?\" #@param {type: \"string\"}\n",
        "print()\n",
        "[print(f'\\r \\n Prediction {i+1}: ' + '\\n' + a['generated_text']+ '\\n', end='') for i, a in enumerate(pipe(text_to_complete, num_return_sequences=4, return_full_text=True, max_new_tokens=250))];"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "gGGXwx4754zk"
      },
      "outputs": [],
      "source": [
        "#@title If you're happy, you can save your model to the huggingface hub by running this cell:\n",
        "trainer.push_to_hub(my_model_name)\n",
        "tokenizer.push_to_hub(my_model_name)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}